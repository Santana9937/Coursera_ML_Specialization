### Week 4 assignments for Coursera Machine Learning: Regression course from the University of Washington.

### Ipython notebooks for Ridge Regression and L2 Penalty

http://nbviewer.ipython.org/github/Santana9937/Regression_ML_Specialization/blob/master/Week_4_Ridge_Regression/assign_1_ridge-regression.ipynb

In this notebook, we will run ridge regression multiple times with different L2 penalties to see which one produces the best fit. We will revisit the example of polynomial regression as a means to see the effect of L2 regularization. In particular, we will:
* Use Sklearn to run polynomial regression
* Use matplotlib to visualize polynomial regressions
* Use Sklearn to run polynomial regression with L2 penalty
* Use matplotlib to visualize polynomial regressions under L2 regularization
* Choose best L2 penalty using cross-validation.
* Assess the final fit using test data.

http://nbviewer.ipython.org/github/Santana9937/Regression_ML_Specialization/blob/master/Week_4_Ridge_Regression/assign_2_ridge-regression.ipynb

In this notebook, we will implement ridge regression via gradient descent. You will:
* Convert an SFrame into a Numpy array
* Write a Numpy function to compute the derivative of the regression weights with respect to a single feature
* Write gradient descent function to compute the regression weights given an initial weight vector, step size, tolerance, and L2 penalty



